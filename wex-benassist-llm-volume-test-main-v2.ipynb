{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of user records: 10\n",
      "Number of questions: 9\n",
      "Error retrieving token for user T_09684202: 400 Client Error: Bad Request for url: https://dv-id.mybenefitexpress.com/connect/token\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 370\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 370\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validate_token(token):\n",
      "Cell \u001b[1;32mIn[1], line 93\u001b[0m, in \u001b[0;36mretrieve_token\u001b[1;34m(client_id)\u001b[0m\n\u001b[0;32m     92\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(auth_url, data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m---> 93\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m token_data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://dv-id.mybenefitexpress.com/connect/token",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[1], line 378\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError retrieving token for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 378\u001b[0m         \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m bedrock_client \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\n\u001b[0;32m    381\u001b[0m     service_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedrock-runtime\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    382\u001b[0m     region_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mus-east-1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m )\n",
      "\u001b[1;31mSystemExit\u001b[0m: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2121\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2119\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2120\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2121\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2122\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2125\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains_exceptiongroup\u001b[39m(val):\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py:710\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    703\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \n\u001b[0;32m    705\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py:568\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    565\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    566\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    567\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 568\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    572\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1435\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1435\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1326\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1323\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1325\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1173\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1166\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1170\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1171\u001b[0m ):\n\u001b[0;32m   1172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1174\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1176\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1177\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1063\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1061\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1062\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1063\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1064\u001b[0m )\n\u001b[0;32m   1066\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1067\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\IPython\\core\\ultratb.py:1131\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1131\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import certifi\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display, clear_output\n",
    "from threading import Lock\n",
    "from datetime import datetime\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import sys\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "import urllib3\n",
    "\n",
    "# Suppress warnings for making HTTPS requests to localhost without verifying SSL certificates.\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Set the AWS profile and certificate bundle\n",
    "os.environ[\"AWS_PROFILE\"] = \"default\"\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "\n",
    "# Create a temporary directory for storing the uploaded files\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "# Set the model ID\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "\n",
    "def validate_file(file_path):\n",
    "    \"\"\"\n",
    "    Validates the uploaded file to ensure it can be read as an Excel file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: str, path to the file to be validated\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple (bool, int): True if the file is valid, and the number of rows in the file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        return True, df.shape[0]\n",
    "    except Exception as e:\n",
    "        return False, 0\n",
    "\n",
    "def validate_token(token):\n",
    "    \"\"\"\n",
    "    Validates the Bearer token by making a test API call.\n",
    "    \n",
    "    Parameters:\n",
    "    - token: str, the Bearer token\n",
    "    \n",
    "    Returns:\n",
    "    - bool: True if the token is valid, False otherwise\n",
    "    \"\"\"\n",
    "    url = 'https://localhost:8443/v1.0/benefitassistant/Session'\n",
    "    headers = {\n",
    "        'accept': '*/*',\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, verify=False)  # verify=False for local testing, remove in production\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n",
    "\n",
    "def retrieve_token(client_id):\n",
    "    \"\"\"\n",
    "    Retrieves the Bearer token using client credentials.\n",
    "    \n",
    "    Parameters:\n",
    "    - client_id: str, the client ID (user ID in this context)\n",
    "    \n",
    "    Returns:\n",
    "    - str: Bearer token\n",
    "    \"\"\"\n",
    "    client_secret = \"14DA6952-20F7-46EC-9342-F3F4BCF8EC1A\"\n",
    "    auth_url = \"https://dv-id.mybenefitexpress.com/connect/token\"\n",
    "\n",
    "    data = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"scope\": \"externalidentityprovider\",\n",
    "        \"client_id\": client_id,\n",
    "        \"client_secret\": client_secret\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(auth_url, data=data)\n",
    "        response.raise_for_status()\n",
    "        token_data = response.json()\n",
    "        return token_data[\"access_token\"]\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error retrieving token for user {client_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def refresh_bedrock_client():\n",
    "    \"\"\"\n",
    "    Refreshes the Bedrock client with new credentials.\n",
    "    \n",
    "    Returns:\n",
    "    - bedrock_client: boto3 client, refreshed Bedrock client, or None if an error occurs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        session = boto3.Session(profile_name=os.environ[\"AWS_PROFILE\"])\n",
    "        credentials = session.get_credentials()\n",
    "        credentials = credentials.get_frozen_credentials()\n",
    "\n",
    "        bedrock_client = boto3.client(\n",
    "            service_name=\"bedrock-runtime\",\n",
    "            region_name=\"us-east-1\",\n",
    "            aws_access_key_id=credentials.access_key,\n",
    "            aws_secret_access_key=credentials.secret_key,\n",
    "            aws_session_token=credentials.token\n",
    "        )\n",
    "        return bedrock_client\n",
    "    except (NoCredentialsError, PartialCredentialsError) as e:\n",
    "        print(f\"Error refreshing AWS credentials: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def chat(question, token):\n",
    "    \"\"\"\n",
    "    Sends a question to the API and returns the response.\n",
    "    \n",
    "    Parameters:\n",
    "    - question: str, the question to be sent\n",
    "    - token: str, the authorization token for the API\n",
    "    \n",
    "    Returns:\n",
    "    - response: dict, the response from the API\n",
    "    \"\"\"\n",
    "    url = f'https://localhost:8443/v1.0/benefitassistant/Assistant/llm-response?question={question}'\n",
    "    headers = {\n",
    "        'accept': '*/*',\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, verify=False)  # verify=False for local testing, remove in production\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def simulate_user_questions(user_id, questions, question_ids, user_progress, user_time, response_times, avg_response_times, error_log, lock, results, token, start_time):\n",
    "    \"\"\"\n",
    "    Simulates a user asking questions and updates the progress.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: str, ID of the user\n",
    "    - questions: list, list of questions to ask\n",
    "    - question_ids: list, list of question IDs corresponding to the questions\n",
    "    - user_progress: dict, dictionary to store user progress\n",
    "    - user_time: dict, dictionary to store time taken by the user\n",
    "    - response_times: list, list of response times for each question\n",
    "    - avg_response_times: list, list of average response times for each question\n",
    "    - error_log: list, list to store error messages\n",
    "    - lock: threading.Lock, lock to synchronize access to shared resources\n",
    "    - results: list, list to store results for the Excel file\n",
    "    - token: str, the authorization token for the API\n",
    "    - start_time: float, the start time of the first question submission\n",
    "    \"\"\"\n",
    "    combined = list(zip(question_ids, questions))\n",
    "    random.shuffle(combined)\n",
    "    for question_id, question in combined:\n",
    "        try:\n",
    "            start_time_question = time.time()\n",
    "            response = chat(question, token)\n",
    "            end_time_question = time.time()\n",
    "            time_taken = end_time_question - start_time_question\n",
    "            user_time[user_id] += time_taken\n",
    "            response_times.append(time_taken)\n",
    "            avg_response_times.append(sum(response_times) / len(response_times))\n",
    "            \n",
    "            if \"error\" in response:\n",
    "                raise Exception(response[\"error\"])\n",
    "            \n",
    "            answer = response.get(\"answer\", \"\")\n",
    "            user_progress[user_id][\"answered\"] += 1\n",
    "            results.append([user_id, question, answer, \"\"])\n",
    "        except Exception as e:\n",
    "            error_info = f\"Error occurred for user {user_id}, question ID {question_id}, question: {question}. Error: {str(e)}\"\n",
    "            error_log.append(error_info)\n",
    "            user_progress[user_id][\"error\"] += 1\n",
    "            results.append([user_id, question, \"\", str(e)])\n",
    "\n",
    "        with lock:\n",
    "            update_plot(user_progress, user_time, response_times, avg_response_times, error_log, start_time)\n",
    "\n",
    "def update_plot(user_progress, user_time, response_times, avg_response_times, error_log, start_time):\n",
    "    \"\"\"\n",
    "    Updates the plot showing the progress of users and other metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_progress: dict, dictionary containing progress of users\n",
    "    - user_time: dict, dictionary containing time taken by users\n",
    "    - response_times: list, list of response times\n",
    "    - avg_response_times: list, list of average response times\n",
    "    - error_log: list, list of error messages\n",
    "    - start_time: float, the start time of the first question submission\n",
    "    \"\"\"\n",
    "    progress_df = pd.DataFrame(user_progress).T\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 15))\n",
    "\n",
    "    # Adjust layout to add space between the plots\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Plot progress of answered questions for each user\n",
    "    progress_df.plot(kind='bar', stacked=True, color=['green', 'red'], ax=axes[0])\n",
    "    axes[0].set_xlabel('User ID')\n",
    "    axes[0].set_ylabel('Number of Questions')\n",
    "    axes[0].set_title('Progress of Answered Questions for Each User')\n",
    "    axes[0].yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    # Annotate the bars with the time taken\n",
    "    for p in axes[0].patches:\n",
    "        width, height = p.get_width(), p.get_height()\n",
    "        if height > 0:\n",
    "            x, y = p.get_xy()\n",
    "            user_id = progress_df.index[int(x + width / 2)]\n",
    "            if user_progress[user_id][\"answered\"] > 0:  # Annotate only once per user\n",
    "                time_text = f\"{user_time[user_id] / 60:.2f} min\"\n",
    "                axes[0].annotate(time_text, (x + width / 2, height), \n",
    "                                 ha='center', va='bottom', fontsize=10, color='black', fontweight='bold')\n",
    "\n",
    "    # Calculate total elapsed time\n",
    "    total_time_taken = (time.time() - start_time) / 60  # in minutes\n",
    "\n",
    "    # Display number of errors, successful answers, and total time taken\n",
    "    total_errors = sum(user['error'] for user in user_progress.values())\n",
    "    total_successes = sum(user['answered'] for user in user_progress.values())\n",
    "    textstr = f'Total Errors: {total_errors}\\nTotal Successful Answers: {total_successes}\\nTotal Time Taken: {total_time_taken:.2f} min'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    axes[0].text(0.05, 0.95, textstr, transform=axes[0].transAxes, fontsize=12,\n",
    "                 verticalalignment='top', bbox=props)\n",
    "\n",
    "    # Plot response times and average response times\n",
    "    axes[1].plot(response_times, label='Response Time', color='orange')\n",
    "    axes[1].plot(avg_response_times, label='Average Response Time', linestyle='--', color='blue')\n",
    "    axes[1].set_xlabel('Request Number')\n",
    "    axes[1].set_ylabel('Response Time (s)')\n",
    "    axes[1].set_title('Response Times')\n",
    "    axes[1].legend()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "def analyze_results(user_count, question_count, total_successes, total_errors, total_time_taken, avg_response_times, response_times, error_log):\n",
    "    \"\"\"\n",
    "    Analyzes the volume test results using the Bedrock LLM.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_count: int, number of users\n",
    "    - question_count: int, number of questions\n",
    "    - total_successes: int, total successful answers\n",
    "    - total_errors: int, total errors\n",
    "    - total_time_taken: float, total time taken for the volume test\n",
    "    - avg_response_times: list, average response times for each request\n",
    "    - response_times: list, response times for each request\n",
    "    - error_log: list, list of error messages\n",
    "    \n",
    "    Returns:\n",
    "    - analysis: str, analysis provided by the LLM or an error message if the analysis fails\n",
    "    \"\"\"\n",
    "    results_summary = f\"\"\"\n",
    "    Volume Testing Results Summary:\n",
    "    - Number of users: {user_count}\n",
    "    - Number of questions: {question_count}\n",
    "    - Total Successful Answers: {total_successes}\n",
    "    - Total Errors: {total_errors}\n",
    "    - Total Time Taken: {total_time_taken:.2f} minutes\n",
    "    - Average Response Time: {sum(avg_response_times) / len(avg_response_times):.2f} seconds\n",
    "    - Maximum Response Time: {max(response_times):.2f} seconds\n",
    "    - Minimum Response Time: {min(response_times):.2f} seconds\n",
    "    \n",
    "    Error Log:\n",
    "    {error_log}\n",
    "    \"\"\"\n",
    "    \n",
    "    question = f\"\"\"\n",
    "    We are testing an API. Please analyze the following volume test results and provide insights on what is good, what problems exist, and how to improve performance if there are any issues.\n",
    "    \n",
    "    {results_summary}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"{question}\"\n",
    "    )\n",
    "\n",
    "    global bedrock_client  # Ensure we use the global bedrock_client\n",
    "    llm = BedrockLLM(\n",
    "        model_id=model_id,\n",
    "        client=bedrock_client,\n",
    "        model_kwargs={\"max_gen_len\": 512, \"temperature\": 0.5}\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        bedrock_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        response = bedrock_chain({'question': question})\n",
    "        return response['text']\n",
    "    except Exception as e:\n",
    "        if \"ExpiredTokenException\" in str(e):\n",
    "            print(\"Token expired. Refreshing token...\")\n",
    "            bedrock_client = refresh_bedrock_client()\n",
    "            if bedrock_client is None:\n",
    "                print(\"Failed to refresh token. Continuing without analysis.\")\n",
    "                return \"Failed to refresh token. Analysis not performed.\"\n",
    "            \n",
    "            llm = BedrockLLM(\n",
    "                model_id=model_id,\n",
    "                client=bedrock_client,\n",
    "                model_kwargs={\"max_gen_len\": 512, \"temperature\": 0.5}\n",
    "            )\n",
    "            bedrock_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "            try:\n",
    "                response = bedrock_chain({'question': question})\n",
    "                return response['text']\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during analysis after token refresh: {str(e)}\")\n",
    "                return f\"An error occurred during analysis after token refresh: {str(e)}\"\n",
    "        else:\n",
    "            print(f\"An error occurred during analysis: {str(e)}\")\n",
    "            return f\"An error occurred during analysis: {str(e)}\"\n",
    "\n",
    "\n",
    "# >>>>>>>>>>> !!! this is starting point of our application\n",
    "\n",
    "# Placeholder for user input file paths\n",
    "users_file_path = \"wex-users.xlsx\"  # Replace with the actual path\n",
    "questions_file_path = \"wex-questions.xlsx\"  # Replace with the actual path\n",
    "\n",
    "# Define your Bearer token here\n",
    "auth_client_secret = \"14DA6952-20F7-46EC-9342-F3F4BCF8EC1A\"  # Fixed client secret\n",
    "\n",
    "# Validate the files\n",
    "is_users_valid, user_count = validate_file(users_file_path)\n",
    "is_questions_valid, question_count = validate_file(questions_file_path)\n",
    "\n",
    "error_log = []\n",
    "response_times = []\n",
    "avg_response_times = []\n",
    "results = []\n",
    "\n",
    "if is_users_valid and is_questions_valid:\n",
    "    users = pd.read_excel(users_file_path)\n",
    "    questions_df = pd.read_excel(questions_file_path)\n",
    "\n",
    "    if 'Question' not in questions_df.columns or 'question-id' not in questions_df.columns:\n",
    "        print(\"Error: The 'question-id' and/or 'Question' column is not found in the questions file.\")\n",
    "        print(f\"Available columns are: {questions_df.columns.tolist()}\")\n",
    "    elif 'user-id' not in users.columns:\n",
    "        print(\"Error: The 'user-id' column is not found in the users file.\")\n",
    "        print(f\"Available columns are: {users.columns.tolist()}\")\n",
    "    else:\n",
    "        questions = questions_df[\"Question\"].tolist()\n",
    "        question_ids = questions_df[\"question-id\"].tolist()\n",
    "        user_ids = users[\"user-id\"].tolist()\n",
    "\n",
    "        print(f\"Number of user records: {user_count}\")\n",
    "        print(f\"Number of questions: {question_count}\")\n",
    "\n",
    "        user_progress = {user_id: {\"answered\": 0, \"error\": 0} for user_id in user_ids}\n",
    "        user_time = {user_id: 0 for user_id in user_ids}\n",
    "        lock = Lock()\n",
    "\n",
    "        tokens = {}\n",
    "        for user_id in user_ids:\n",
    "            try:\n",
    "                token = retrieve_token(user_id)\n",
    "                if validate_token(token):\n",
    "                    tokens[user_id] = token\n",
    "                else:\n",
    "                    print(f\"Error: Invalid Bearer token for user {user_id}.\")\n",
    "                    sys.exit(1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving token for user {user_id}: {str(e)}\")\n",
    "                sys.exit(1)\n",
    "\n",
    "        bedrock_client = boto3.client(\n",
    "            service_name=\"bedrock-runtime\",\n",
    "            region_name=\"us-east-1\"\n",
    "        )\n",
    "\n",
    "        llm = BedrockLLM(\n",
    "            model_id=model_id,\n",
    "            client=bedrock_client,\n",
    "            model_kwargs={\"max_gen_len\": 512, \"temperature\": 0.5}\n",
    "        )\n",
    "\n",
    "        start_time = time.time()  # Capture the start time\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=len(user_ids)) as executor:\n",
    "            futures = []\n",
    "            for user_id in user_ids:\n",
    "                token = tokens[user_id]\n",
    "                futures.append(executor.submit(simulate_user_questions, user_id, questions, question_ids, user_progress, user_time, response_times, avg_response_times, error_log, lock, results, token, start_time))\n",
    "                time.sleep(5)\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                future.result()\n",
    "\n",
    "        total_time_taken = (time.time() - start_time) / 60  # Calculate the total elapsed time\n",
    "\n",
    "        print(f\"Volume testing completed with {user_count} users and {question_count} questions.\")\n",
    "        if error_log:\n",
    "            print(\"Errors occurred during volume testing. Here are the details:\")\n",
    "            for error in error_log:\n",
    "                print(error)\n",
    "\n",
    "        try:\n",
    "            # Analyze the results using the LLM\n",
    "            total_successes = sum(user['answered'] for user in user_progress.values())\n",
    "            total_errors = sum(user['error'] for user in user_progress.values())\n",
    "            analysis = analyze_results(len(user_ids), len(questions), total_successes, total_errors, total_time_taken, avg_response_times, response_times, error_log)\n",
    "            print(\"\\nLLM Analysis and Recommendations:\\n\")\n",
    "            print(analysis)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during analysis: {str(e)}\")\n",
    "\n",
    "        # Save results to an Excel file\n",
    "        results_df = pd.DataFrame(results, columns=['user-id', 'question', 'answer', 'error'])\n",
    "        timestamp = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "        results_df.to_excel(f'volume-test-run-{timestamp}.xlsx', index=False)\n",
    "        print(f\"Results saved to volume-test-run-{timestamp}.xlsx\")\n",
    "else:\n",
    "    print(\"One or both of the uploaded files are not valid Excel files.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
