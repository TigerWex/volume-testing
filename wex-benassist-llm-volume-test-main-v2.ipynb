{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyJhbGciOiJSUzI1NiIsImtpZCI6IjEzNDAxMURGMjAwRENGOTlCN0M2RkMzMDNFMEQ4REVBMUM5QjYzRjAiLCJ0eXAiOiJKV1QiLCJ4NXQiOiJFMEFSM3lBTno1bTN4dnd3UGcyTjZoeWJZX0EifQ.eyJuYmYiOjE3MTcwNzkxMTcsImV4cCI6MTcxNzA4MDAxNywiaXNzIjoiaHR0cHM6Ly9kdi1pZC5teWJlbmVmaXRleHByZXNzLmNvbSIsImF1ZCI6WyJodHRwczovL2R2LWlkLm15YmVuZWZpdGV4cHJlc3MuY29tL3Jlc291cmNlcyIsImFwaSJdLCJjbGllbnRfaWQiOiJtYmU1MCIsImNsaWVudF9ncmFudF90eXBlIjoiY2xpZW50X2NyZWRlbnRpYWxzIiwiY2xpZW50X3VzZXJpZCI6Ijk2ODQyMDIiLCJjbGllbnRfdGVuYW50X2lkIjoiMTEzMCIsImNsaWVudF9jbGllbnRfaWQiOiJtYmU1MCIsImNsaWVudF9jbGllbnRfc2VjcmV0IjoiMDEyMjZDNUYtNDA0Ny00OEVELUJFNkYtNTc0OTUyQzhFQkJGIiwiY2xpZW50X3Njb3BlIjoiZXh0ZXJuYWxpZGVudGl0eXByb3ZpZGVyIiwic2NvcGUiOlsiZXh0ZXJuYWxpZGVudGl0eXByb3ZpZGVyIl19.fY_PPpzSh4sHHQ8ND5BjQmdlF8AsvOy-6yc9CUHOJ11N2sReD0SSpdB-jLELNyb5PMtEzxrf1nW0YyKOxKDVpin0ySqutuz0KvkFuA54lzB40yYeg0Ocn52g91owMt4NM6QJwGOR1pUyKOQXjSL3IoHcrIkR6IwjTbmiOCScOJSAyE5BNqBpWIQJYRn5NmXuXhyT0Vg09aCWDB_a_7ZILnqFL3td6_BSpPZ-EUeF4om0gNOy2e_mN7Zct-UjAyQfjeGxW4bUXUNW4Ax7uqbjw8AilfdeLp-3JK_yeVwUYCmeNEU1nf1IWjXCO-bYimp6GqtVBdFz3azDTfXIKUlynw\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 298\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError retrieving token for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Proceed with the volume test using the tokens\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43muser_tokens\u001b[49m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid tokens retrieved. Exiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    300\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 298\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError retrieving token for user \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;66;03m# Proceed with the volume test using the tokens\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43muser_tokens\u001b[49m:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid tokens retrieved. Exiting...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    300\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[1;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[0;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[0;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import certifi\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display, clear_output\n",
    "from threading import Lock\n",
    "from datetime import datetime\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import sys\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "import urllib3\n",
    "import json\n",
    "\n",
    "# Suppress only the single InsecureRequestWarning from urllib3 needed\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Set the AWS profile and certificate bundle\n",
    "os.environ[\"AWS_PROFILE\"] = \"default\"\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "\n",
    "# bedrock model\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "\n",
    "# Configuration values\n",
    "auth_client_secret = \"01226C5F-4047-48ED-BE6F-574952C8EBBF\"\n",
    "auth_url = \"https://dv-id.mybenefitexpress.com/connect/token\"\n",
    "NUM_QUESTIONS_PER_USER = 10  # Constant number of questions each user will take\n",
    "\n",
    "def get_token_async():\n",
    "    token_request = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"userid\": \"9684202\",\n",
    "        \"tenant_id\": \"1130\",\n",
    "        \"client_id\": \"mbe50\",\n",
    "        \"client_secret\": \"01226C5F-4047-48ED-BE6F-574952C8EBBF\",\n",
    "        \"scope\": \"externalidentityprovider\"\n",
    "    }\n",
    "\n",
    "    token_response = requests.post(\"https://dv-id.mybenefitexpress.com/connect/token\", data=token_request)\n",
    "    token_json = token_response.text\n",
    "\n",
    "    token_data = json.loads(token_json)\n",
    "\n",
    "    if \"access_token\" in token_data:\n",
    "        return token_data[\"access_token\"]\n",
    "    else:\n",
    "        raise Exception(\"Unable to retrieve access token from IdentityServer.\")\n",
    "\n",
    "token = get_token_async()\n",
    "print(token)\n",
    "\n",
    "\n",
    "def validate_file(file_path):\n",
    "   \n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        return True, df.shape[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred during file validation: {e}\")\n",
    "        return False, 0\n",
    "\n",
    "def validate_token(token):\n",
    "    \n",
    "    url = 'https://benefit-assistant.dv.mybenefitexpress.com/v1.0/benefitassistant/Session'\n",
    "    headers = {\n",
    "        'accept': '*/*',\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, verify=False)  # verify=False for local testing, remove in production\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Exception occurred during token validation: {e}\")\n",
    "        return False\n",
    "\n",
    "def retrieve_token(user_id, client_id, client_secret, auth_url):\n",
    "    \n",
    "    data = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"userid\": user_id,\n",
    "        \"tenant_id\": client_id,\n",
    "        \"logged_in_tenant_id\": client_id,\n",
    "        \"logged_in_user_id\": user_id,\n",
    "        \"client_id\": \"mbe50\",\n",
    "        \"client_secret\": client_secret,\n",
    "        \"scope\": \"externalidentityprovider\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(auth_url, data=data)\n",
    "        response.raise_for_status()\n",
    "        token_data = response.json()\n",
    "        \n",
    "        if \"access_token\" in token_data:\n",
    "            token = token_data['access_token']\n",
    "            return token.strip(\"'\")  # Strip any surrounding single quotes from the token\n",
    "        else:\n",
    "            raise Exception(\"Unable to retrieve access token from IdentityServer.\")\n",
    "    except requests.RequestException as ex:\n",
    "        print(f\"An error occurred during authentication: {ex}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def chat(question, token):\n",
    "   \n",
    "    url = f'https://benefit-assistant.dv.mybenefitexpress.com/v1.0/benefitassistant/Assistant/llm-response?question={question}'\n",
    "    headers = {\n",
    "        'accept': '*/*',\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, verify=False)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def simulate_user_questions(user_id, questions, question_ids, user_progress, user_time, response_times, avg_response_times, error_log, lock, results, token, start_time, num_questions_per_user):\n",
    "   \n",
    "    combined = list(zip(question_ids, questions))\n",
    "    random.shuffle(combined)\n",
    "    selected_questions = random.sample(combined, min(num_questions_per_user, len(combined)))\n",
    "    \n",
    "    for question_id, question in selected_questions:\n",
    "        try:\n",
    "            start_time_question = time.time()\n",
    "            response = chat(question, token)\n",
    "            end_time_question = time.time()\n",
    "            time_taken = end_time_question - start_time_question\n",
    "            user_time[user_id] += time_taken\n",
    "            response_times.append(time_taken)\n",
    "            avg_response_times.append(sum(response_times) / len(response_times))\n",
    "            \n",
    "            if \"error\" in response:\n",
    "                raise Exception(response[\"error\"])\n",
    "            \n",
    "            answer = response.get(\"answer\", \"\")\n",
    "            user_progress[user_id][\"answered\"] += 1\n",
    "            results.append([user_id, question, answer, \"\"])\n",
    "        except Exception as e:\n",
    "            error_info = f\"Error occurred for user {user_id}, question ID {question_id}, question: {question}. Error: {str(e)}\"\n",
    "            error_log.append(error_info)\n",
    "            user_progress[user_id][\"error\"] += 1\n",
    "            results.append([user_id, question, \"\", str(e)])\n",
    "\n",
    "        with lock:\n",
    "            update_plot(user_progress, user_time, response_times, avg_response_times, error_log, start_time)\n",
    "\n",
    "def update_plot(user_progress, user_time, response_times, avg_response_times, error_log, start_time):\n",
    "    \n",
    "    progress_df = pd.DataFrame(user_progress).T\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 15))\n",
    "\n",
    "    # Adjust layout to add space between the plots\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Plot progress of answered questions for each user\n",
    "    progress_df.plot(kind='bar', stacked=True, color=['green', 'red'], ax=axes[0])\n",
    "    axes[0].set_xlabel('User ID')\n",
    "    axes[0].set_ylabel('Number of Questions')\n",
    "    axes[0].set_title('Progress of Answered Questions for Each User')\n",
    "    axes[0].yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    # Annotate the bars with the time taken\n",
    "    for p in axes[0].patches:\n",
    "        width, height = p.get_width(), p.get_height()\n",
    "        if height > 0:\n",
    "            x, y = p.get_xy()\n",
    "            user_id = progress_df.index[int(x + width / 2)]\n",
    "            if user_progress[user_id][\"answered\"] > 0:  # Annotate only once per user\n",
    "                time_text = f\"{user_time[user_id] / 60:.2f} min\"\n",
    "                axes[0].annotate(time_text, (x + width / 2, height), \n",
    "                                 ha='center', va='bottom', fontsize=10, color='black', fontweight='bold')\n",
    "\n",
    "    # Calculate total elapsed time\n",
    "    total_time_taken = (time.time() - start_time) / 60  # in minutes\n",
    "\n",
    "    # Display number of errors, successful answers, and total time taken\n",
    "    total_errors = sum(user['error'] for user in user_progress.values())\n",
    "    total_successes = sum(user['answered'] for user in user_progress.values())\n",
    "    textstr = f'Total Errors: {total_errors}\\nTotal Successful Answers: {total_successes}\\nTotal Time Taken: {total_time_taken:.2f} min'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    axes[0].text(0.05, 0.95, textstr, transform=axes[0].transAxes, fontsize=12,\n",
    "                 verticalalignment='top', bbox=props)\n",
    "\n",
    "    # Plot response times and average response times\n",
    "    axes[1].plot(response_times, label='Response Time', color='orange')\n",
    "    axes[1].plot(avg_response_times, label='Average Response Time', linestyle='--', color='blue')\n",
    "    axes[1].set_xlabel('Request Number')\n",
    "    axes[1].set_ylabel('Response Time (s)')\n",
    "    axes[1].set_title('Response Times')\n",
    "    axes[1].legend()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "def analyze_results(user_count, question_count, total_successes, total_errors, total_time_taken, avg_response_times, response_times, error_log):\n",
    "    \n",
    "    results_summary = f\"\"\"\n",
    "    Volume Testing Results Summary:\n",
    "    - Number of users: {user_count}\n",
    "    - Number of questions: {question_count}\n",
    "    - Total Successful Answers: {total_successes}\n",
    "    - Total Errors: {total_errors}\n",
    "    - Total Time Taken: {total_time_taken:.2f} minutes\n",
    "    - Average Response Time: {sum(avg_response_times) / len(avg_response_times):.2f} seconds\n",
    "    - Maximum Response Time: {max(response_times):.2f} seconds\n",
    "    - Minimum Response Time: {min(response_times):.2f} seconds\n",
    "    \n",
    "    Error Log:\n",
    "    {error_log}\n",
    "    \"\"\"\n",
    "    \n",
    "    question = f\"\"\"\n",
    "    We are testing an API. Please analyze the following volume test results and provide insights on what is good, what problems exist, and how to improve performance if there are any issues.\n",
    "    \n",
    "    {results_summary}\n",
    "    \"\"\"\n",
    "    \n",
    "    global bedrock_client  # Ensure we use the global bedrock_client\n",
    "    llm = BedrockLLM(\n",
    "        model_id=model_id,\n",
    "        client=bedrock_client,\n",
    "        model_kwargs={\"max_gen_len\": 512, \"temperature\": 0.5}\n",
    "    )\n",
    "\n",
    "    prompt = PromptTemplate(template=question)\n",
    "\n",
    "    try:\n",
    "        bedrock_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        response = bedrock_chain({'question': question})\n",
    "        return response['text']\n",
    "    except Exception as e:\n",
    "        if \"ExpiredTokenException\" in str(e):\n",
    "            print(\"Token expired. Refreshing token...\")\n",
    "            bedrock_client = refresh_bedrock_client()\n",
    "            if bedrock_client is None:\n",
    "                print(\"Failed to refresh token. Continuing without analysis.\")\n",
    "                return \"Failed to refresh token. Analysis not performed.\"\n",
    "            \n",
    "            llm = BedrockLLM(\n",
    "                model_id=model_id,\n",
    "                client=bedrock_client,\n",
    "                model_kwargs={\"max_gen_len\": 512, \"temperature\": 0.5}\n",
    "            )\n",
    "            bedrock_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "            try:\n",
    "                response = bedrock_chain({'question': question})\n",
    "                return response['text']\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred during analysis after token refresh: {str(e)}\")\n",
    "                return f\"An error occurred during analysis after token refresh: {str(e)}\"\n",
    "        else:\n",
    "            print(f\"An error occurred during analysis: {str(e)}\")\n",
    "            return f\"An error occurred during analysis: {str(e)}\"\n",
    "\n",
    "# Placeholder for user input file paths\n",
    "users_file_path = \"wex-users.xlsx\"  # Replace with the actual path\n",
    "questions_file_path = \"wex-questions.xlsx\"  # Replace with the actual path\n",
    "\n",
    "# Validate the files\n",
    "is_users_valid, user_count = validate_file(users_file_path)\n",
    "is_questions_valid, question_count = validate_file(questions_file_path)\n",
    "\n",
    "if not is_users_valid or not is_questions_valid:\n",
    "    print(\"One or both of the uploaded files are not valid Excel files.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Load user data from the users file\n",
    "users = pd.read_excel(users_file_path)\n",
    "user_ids = users[\"user-id\"].tolist()\n",
    "client_ids = users[\"client-id\"].tolist()\n",
    "\n",
    "# Initialize a dictionary to store tokens\n",
    "user_tokens = {}\n",
    "\n",
    "# Retrieve and validate tokens for each user\n",
    "for user_id, client_id in zip(user_ids, client_ids):\n",
    "    try:\n",
    "        token = retrieve_token(user_id, client_id, auth_client_secret, auth_url)\n",
    "        if validate_token(token):\n",
    "            user_tokens[user_id] = token\n",
    "        else:\n",
    "            print(f\"Error: Invalid Bearer token for user {user_id}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving token for user {user_id}: {str(e)}\")\n",
    "\n",
    "# Proceed with the volume test using the tokens\n",
    "if not user_tokens:\n",
    "    print(\"No valid tokens retrieved. Exiting...\")\n",
    "    sys.exit(1)\n",
    "\n",
    "error_log = []\n",
    "response_times = []\n",
    "avg_response_times = []\n",
    "results = []\n",
    "\n",
    "users = pd.read_excel(users_file_path)\n",
    "questions_df = pd.read_excel(questions_file_path)\n",
    "\n",
    "if 'Question' not in questions_df.columns or 'question-id' not in questions_df.columns:\n",
    "    print(\"Error: The 'question-id' and/or 'Question' column is not found in the questions file.\")\n",
    "    print(f\"Available columns are: {questions_df.columns.tolist()}\")\n",
    "    sys.exit(1)\n",
    "elif 'user-id' not in users.columns:\n",
    "    print(\"Error: The 'user-id' column is not found in the users file.\")\n",
    "    print(f\"Available columns are: {users.columns.tolist()}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    questions = questions_df[\"Question\"].tolist()\n",
    "    question_ids = questions_df[\"question-id\"].tolist()\n",
    "    user_ids = users[\"user-id\"].tolist()\n",
    "\n",
    "    print(f\"Number of user records: {user_count}\")\n",
    "    print(f\"Number of questions: {question_count}\")\n",
    "\n",
    "    user_progress = {user_id: {\"answered\": 0, \"error\": 0} for user_id in user_ids}\n",
    "    user_time = {user_id: 0 for user_id in user_ids}\n",
    "    lock = Lock()\n",
    "\n",
    "    bedrock_client = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "\n",
    "    llm = BedrockLLM(\n",
    "        model_id=model_id,\n",
    "        client=bedrock_client,\n",
    "        model_kwargs={\"max_gen_len\": 512, \"temperature\": 0.5}\n",
    "    )\n",
    "\n",
    "    start_time = time.time()  # Capture the start time\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=len(user_ids)) as executor:\n",
    "        futures = []\n",
    "        for user_id in user_ids:\n",
    "            token = user_tokens.get(user_id)\n",
    "            if token:\n",
    "                futures.append(executor.submit(simulate_user_questions, user_id, questions, question_ids, user_progress, user_time, response_times, avg_response_times, error_log, lock, results, token, start_time, NUM_QUESTIONS_PER_USER))\n",
    "                time.sleep(5)\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "    total_time_taken = (time.time() - start_time) / 60  # Calculate the total elapsed time\n",
    "\n",
    "    print(f\"Volume testing completed with {user_count} users and {question_count} questions.\")\n",
    "    if error_log:\n",
    "        print(\"Errors occurred during volume testing. Here are the details:\")\n",
    "        for error in error_log:\n",
    "            print(error)\n",
    "\n",
    "    try:\n",
    "        # Analyze the results using the LLM\n",
    "        total_successes = sum(user['answered'] for user in user_progress.values())\n",
    "        total_errors = sum(user['error'] for user in user_progress.values())\n",
    "        analysis = analyze_results(len(user_ids), len(questions), total_successes, total_errors, total_time_taken, avg_response_times, response_times, error_log)\n",
    "        print(\"\\nLLM Analysis and Recommendations:\\n\")\n",
    "        print(analysis)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during analysis: {str(e)}\")\n",
    "\n",
    "    # Save results to an Excel file\n",
    "    results_df = pd.DataFrame(results, columns=['user-id', 'question', 'answer', 'error'])\n",
    "    timestamp = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "    results_df.to_excel(f'volume-test-run-{timestamp}.xlsx', index=False)\n",
    "    print(f\"Results saved to volume-test-run-{timestamp}.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
