{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token retrieved: eyJhbGciOiJSUzI1NiIsImtpZCI6IjEzNDAxMURGMjAwRENGOTlCN0M2RkMzMDNFMEQ4REVBMUM5QjYzRjAiLCJ0eXAiOiJKV1QiLCJ4NXQiOiJFMEFSM3lBTno1bTN4dnd3UGcyTjZoeWJZX0EifQ.eyJuYmYiOjE3MTgwNTEzMjksImV4cCI6MTcxODA1MjIyOSwiaXNzIjoiaHR0cHM6Ly9kdi1pZC5teWJlbmVmaXRleHByZXNzLmNvbSIsImF1ZCI6WyJodHRwczovL2R2LWlkLm15YmVuZWZpdGV4cHJlc3MuY29tL3Jlc291cmNlcyIsImFwaSJdLCJjbGllbnRfaWQiOiJtYmU1MCIsImNsaWVudF9ncmFudF90eXBlIjoiY2xpZW50X2NyZWRlbnRpYWxzIiwiY2xpZW50X3VzZXJpZCI6Ijk2ODQyMDIiLCJjbGllbnRfdGVuYW50X2lkIjoiMTEzMCIsImNsaWVudF9jbGllbnRfaWQiOiJtYmU1MCIsImNsaWVudF9jbGllbnRfc2VjcmV0IjoiMDEyMjZDNUYtNDA0Ny00OEVELUJFNkYtNTc0OTUyQzhFQkJGIiwiY2xpZW50X3Njb3BlIjoiZXh0ZXJuYWxpZGVudGl0eXByb3ZpZGVyIiwic2NvcGUiOlsiZXh0ZXJuYWxpZGVudGl0eXByb3ZpZGVyIl19.LhZBHaYtD9zq31dd8ZGTKpedAQfrydObq43UlMC0lW6M4IUfxmr5GJd0b6PxsP0kcX5Y1n7sVIwnI2FZi3zfwtQ043RbgNMnSmo0-uH9XIN1M6ZLAuqrx4xexoizz1L5qEZsHxWKWXxNdPX9hCHnvrDYs0Wm05HbN5oxb3FzYx9611NKpT2AVc0c0zVzQOizPFe_TkVhoKP8V9MCyvM6EhLbTFF8VXhqF8sHJIRNRDyeb9rhIW81FV4aDcjkD2-UYPATlDHJ1g2opJk1WYQha7Nbsp03mmM2KF1q4hqZFMt6w0R1cLLhanUgdzkDTArzuaF-I3pPX5gx6B6aUSuFKw\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 442\u001b[0m\n\u001b[0;32m    439\u001b[0m     plt\u001b[38;5;241m.\u001b[39mclose(fig)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m main()\n",
      "Cell \u001b[1;32mIn[7], line 379\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    376\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_id, token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(user_ids, tokens):\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m validate_token(token):\n\u001b[0;32m    380\u001b[0m         user_tokens[user_id] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[7], line 91\u001b[0m, in \u001b[0;36mvalidate_token\u001b[1;34m(token)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m session\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders, ssl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     92\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\aiohttp\\client.py:1197\u001b[0m, in \u001b[0;36m_BaseRequestContextManager.__aenter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1196\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__aenter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _RetType:\n\u001b[1;32m-> 1197\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coro\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resp\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\aiohttp\\client.py:581\u001b[0m, in \u001b[0;36mClientSession._request\u001b[1;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m ceil_timeout(\n\u001b[0;32m    577\u001b[0m         real_timeout\u001b[38;5;241m.\u001b[39mconnect,\n\u001b[0;32m    578\u001b[0m         ceil_threshold\u001b[38;5;241m=\u001b[39mreal_timeout\u001b[38;5;241m.\u001b[39mceil_threshold,\n\u001b[0;32m    579\u001b[0m     ):\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 581\u001b[0m         conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connector\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[0;32m    582\u001b[0m             req, traces\u001b[38;5;241m=\u001b[39mtraces, timeout\u001b[38;5;241m=\u001b[39mreal_timeout\n\u001b[0;32m    583\u001b[0m         )\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mTimeoutError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServerTimeoutError(\n\u001b[0;32m    586\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection timeout \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto host \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(url)\n\u001b[0;32m    587\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\aiohttp\\connector.py:544\u001b[0m, in \u001b[0;36mBaseConnector.connect\u001b[1;34m(self, req, traces, timeout)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m trace\u001b[38;5;241m.\u001b[39msend_connection_create_start()\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 544\u001b[0m     proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection(req, traces, timeout)\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed:\n\u001b[0;32m    546\u001b[0m         proto\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\aiohttp\\connector.py:944\u001b[0m, in \u001b[0;36mTCPConnector._create_connection\u001b[1;34m(self, req, traces, timeout)\u001b[0m\n\u001b[0;32m    942\u001b[0m     _, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_proxy_connection(req, traces, timeout)\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 944\u001b[0m     _, proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_direct_connection(req, traces, timeout)\n\u001b[0;32m    946\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m proto\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\aiohttp\\connector.py:1203\u001b[0m, in \u001b[0;36mTCPConnector._create_direct_connection\u001b[1;34m(self, req, traces, timeout, client_error)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m port \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1200\u001b[0m     \u001b[38;5;66;03m# Cancelling this lookup should not cancel the underlying lookup\u001b[39;00m\n\u001b[0;32m   1201\u001b[0m     \u001b[38;5;66;03m#  or else the cancel event will get broadcast to all the waiters\u001b[39;00m\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;66;03m#  across all connections.\u001b[39;00m\n\u001b[1;32m-> 1203\u001b[0m     hosts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_host(host, port, traces\u001b[38;5;241m=\u001b[39mtraces)\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, asyncio\u001b[38;5;241m.\u001b[39mTimeoutError):\n",
      "File \u001b[1;32mc:\\Users\\W513032\\source\\repos\\volume testing\\.venv\\lib\\site-packages\\aiohttp\\connector.py:880\u001b[0m, in \u001b[0;36mTCPConnector._resolve_host\u001b[1;34m(self, host, port, traces)\u001b[0m\n\u001b[0;32m    876\u001b[0m resolved_host_task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_host_with_throttle(key, host, port, traces)\n\u001b[0;32m    878\u001b[0m )\n\u001b[0;32m    879\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mshield(resolved_host_task)\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mCancelledError:\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop_exception\u001b[39m(fut: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.Future[List[Dict[str, Any]]]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\asyncio\\futures.py:284\u001b[0m, in \u001b[0;36mFuture.__await__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_asyncio_future_blocking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mawait wasn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt used with future\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\asyncio\\tasks.py:328\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 328\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Python39_64\\lib\\asyncio\\futures.py:196\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m _CANCELLED:\n\u001b[0;32m    195\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_cancelled_error()\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m!=\u001b[39m _FINISHED:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mInvalidStateError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResult is not ready.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import certifi\n",
    "import matplotlib.pyplot as plt\n",
    "from threading import Lock\n",
    "from datetime import datetime\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import sys\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError, PartialCredentialsError\n",
    "import json\n",
    "from IPython.display import display, clear_output\n",
    "import urllib3\n",
    "import time\n",
    "import statistics\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "os.environ[\"AWS_PROFILE\"] = \"default\"\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "\n",
    "# Get environment variables\n",
    "AUTH_CLIENT_SECRET = os.getenv(\"AUTH_CLIENT_SECRET\")\n",
    "AUTH_URL = os.getenv(\"AUTH_URL\")\n",
    "\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "NUM_QUESTIONS_PER_USER = 5\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Dictionary of special response strings\n",
    "SPECIAL_RESPONSES = {\n",
    "    \"ContextUnableProcessMessage\": \"We're unable to process your request at this time.\",\n",
    "    \"LLMUnableProcessMessage\": \"I am unable to respond to your message at this time.\"\n",
    "}\n",
    "\n",
    "def validate_file(file_path):\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        return True, df.shape[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Exception occurred during file validation: {e}\")\n",
    "        return False, 0\n",
    "\n",
    "async def get_token_async():\n",
    "    token_request = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"userid\": \"9684202\",\n",
    "        \"tenant_id\": \"1130\",\n",
    "        \"client_id\": \"mbe50\",\n",
    "        \"client_secret\": AUTH_CLIENT_SECRET,\n",
    "        \"scope\": \"externalidentityprovider\"\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(\"https://dv-id.mybenefitexpress.com/connect/token\", data=token_request) as response:\n",
    "            token_json = await response.text()\n",
    "            token_data = json.loads(token_json)\n",
    "\n",
    "            if \"access_token\" in token_data:\n",
    "                return token_data[\"access_token\"]\n",
    "            else:\n",
    "                raise Exception(\"Unable to retrieve access token from IdentityServer.\")\n",
    "            \n",
    "# Call this test function to get the token\n",
    "try:\n",
    "    token = await get_token_async()\n",
    "    print(f\"Token retrieved: {token}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while retrieving the token: {str(e)}\")\n",
    "\n",
    "async def validate_token(token):\n",
    "    url = 'https://benefit-assistant.dv.mybenefitexpress.com/v1.0/benefitassistant/Session'\n",
    "    headers = {\n",
    "        'accept': '*/*',\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(url, headers=headers, ssl=True) as response:\n",
    "                return response.status == 200\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Exception occurred during token validation: {e}\")\n",
    "            return False\n",
    "\n",
    "async def retrieve_token(user_id, client_id, client_secret, auth_url):\n",
    "    data = {\n",
    "        \"grant_type\": \"client_credentials\",\n",
    "        \"userid\": user_id,\n",
    "        \"tenant_id\": client_id,\n",
    "        \"logged_in_tenant_id\": client_id,\n",
    "        \"logged_in_user_id\": user_id,\n",
    "        \"client_id\": \"mbe50\",\n",
    "        \"client_secret\": client_secret,\n",
    "        \"scope\": \"externalidentityprovider\"\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(auth_url, data=data) as response:\n",
    "                response.raise_for_status()\n",
    "                token_data = await response.json()\n",
    "\n",
    "                if \"access_token\" in token_data:\n",
    "                    return token_data['access_token'].strip(\"'\")\n",
    "                else:\n",
    "                    raise Exception(\"Unable to retrieve access token from IdentityServer.\")\n",
    "        except aiohttp.ClientError as ex:\n",
    "            print(f\"An error occurred during authentication: {ex}\")\n",
    "            return None\n",
    "\n",
    "async def chat(question, token):\n",
    "    url = f'https://benefit-assistant.dv.mybenefitexpress.com/v1.0/benefitassistant/Assistant/llm-response?question={question}'\n",
    "    headers = {\n",
    "        'accept': '*/*',\n",
    "        'Authorization': f'Bearer {token}'\n",
    "    }\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(url, headers=headers, ssl=True) as response:\n",
    "                response.raise_for_status()\n",
    "                return await response.json()\n",
    "        except aiohttp.ClientError as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "async def simulate_user_questions(user_id, questions, question_ids, user_progress, user_time, response_times, avg_response_times, error_log, lock, results, token, start_time, num_questions_per_user, fig, axes):\n",
    "    combined = list(zip(question_ids, questions))\n",
    "    random.shuffle(combined)\n",
    "    selected_questions = random.sample(combined, min(num_questions_per_user, len(combined)))\n",
    "\n",
    "    num_questions_answered = 0\n",
    "\n",
    "    for question_id, question in selected_questions:\n",
    "        try:\n",
    "            start_time_question = datetime.now()\n",
    "            response = await chat(question, token)\n",
    "            end_time_question = datetime.now()\n",
    "            time_taken = (end_time_question - start_time_question).total_seconds()\n",
    "            user_time[user_id] += time_taken\n",
    "            response_times.append(time_taken)\n",
    "            avg_response_times.append(sum(response_times) / len(response_times))\n",
    "\n",
    "            if \"error\" in response:\n",
    "                raise Exception(response[\"error\"])\n",
    "\n",
    "            answer = response.get(\"answer\", \"\")\n",
    "            if answer == SPECIAL_RESPONSES[\"ContextUnableProcessMessage\"]:\n",
    "                user_progress[user_id][\"context_unable_to_process\"] += 1\n",
    "            elif answer == SPECIAL_RESPONSES[\"LLMUnableProcessMessage\"]:\n",
    "                user_progress[user_id][\"llm_unable_to_process\"] += 1\n",
    "            else:\n",
    "                user_progress[user_id][\"answered\"] += 1\n",
    "\n",
    "            results.append([user_id, question, answer, \"\", start_time_question, end_time_question, question_id])\n",
    "            num_questions_answered += 1\n",
    "        except Exception as e:\n",
    "            error_info = f\"Error occurred for user {user_id}, question ID {question_id}, question: {question}. Error: {str(e)}\"\n",
    "            error_log.append(error_info)\n",
    "            user_progress[user_id][\"error\"] += 1\n",
    "            results.append([user_id, question, \"\", str(e), start_time_question, end_time_question, question_id])\n",
    "\n",
    "        with lock:\n",
    "            update_plot(user_progress, user_time, response_times, avg_response_times, error_log, start_time, fig, axes, results)\n",
    "\n",
    "    return num_questions_answered\n",
    "\n",
    "def initialize_plot():\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 15))\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Default values for initial display\n",
    "    total_errors = 0\n",
    "    total_successes = 0\n",
    "    total_context_unable_to_process = 0\n",
    "    total_llm_unable_to_process = 0\n",
    "    total_time_taken = 0\n",
    "\n",
    "    textstr = f'Total Errors: {total_errors}\\nTotal Successful Answers: {total_successes}\\nTotal Context Unable to Process: {total_context_unable_to_process}\\nTotal LLM Unable to Process: {total_llm_unable_to_process}\\nTotal Time Taken: {total_time_taken:.2f} min'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    axes[0].text(0.05, 0.95, textstr, transform=axes[0].transAxes, fontsize=12,\n",
    "                 verticalalignment='top', bbox=props)\n",
    "\n",
    "    return fig, axes\n",
    "\n",
    "\n",
    "'''\n",
    "The update_plot function is called within the simulate_user_questions function each time a question is answered \n",
    "(successfully or with an error). This allows the plot to be updated in real-time as each user's progress is tracked. \n",
    "The function is called inside a with lock block to ensure thread safety when updating shared resources like the plot.\n",
    "\n",
    "The main function initializes the plot, displays it once initially, and then calls simulate_user_questions for each user concurrently. \n",
    "The update_plot function is then responsible for updating the plot with the latest data during each iteration of question processing.\n",
    "'''\n",
    "def update_plot(user_progress, user_time, response_times, avg_response_times, error_log, start_time, fig, axes, results):\n",
    "    progress_df = pd.DataFrame(user_progress).T\n",
    "\n",
    "    axes[0].cla()\n",
    "    progress_df.plot(kind='bar', stacked=True, color=['green', 'red', 'pink', 'yellow'], ax=axes[0])\n",
    "    axes[0].set_xlabel('User ID')\n",
    "    axes[0].set_ylabel('Number of Questions')\n",
    "    axes[0].set_title('Progress of Answered Questions for Each User')\n",
    "    axes[0].yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    question_ids_by_user = {user_id: [] for user_id in user_progress.keys()}\n",
    "    for result in results:\n",
    "        user_id = result[0]\n",
    "        question_id = result[6]\n",
    "        question_ids_by_user[user_id].append(question_id)\n",
    "\n",
    "    annotation_color = 'blue'  # Choose a color other than black for the annotations\n",
    "\n",
    "    for user_id, user_data in user_progress.items():\n",
    "        answered_height = user_data[\"answered\"]\n",
    "        error_height = user_data[\"error\"]\n",
    "        context_unable_to_process_height = user_data[\"context_unable_to_process\"]\n",
    "        llm_unable_to_process_height = user_data[\"llm_unable_to_process\"]\n",
    "        total_height = answered_height + error_height + context_unable_to_process_height + llm_unable_to_process_height\n",
    "\n",
    "        current_height = 0\n",
    "\n",
    "        if answered_height > 0:\n",
    "            for i in range(answered_height):\n",
    "                question_id = question_ids_by_user[user_id][i]\n",
    "                axes[0].annotate(question_id, (progress_df.index.get_loc(user_id), current_height + 0.5), \n",
    "                                 ha='center', va='center', fontsize=8, color=annotation_color, fontweight='bold')\n",
    "                current_height += 1\n",
    "\n",
    "        if error_height > 0:\n",
    "            for i in range(error_height):\n",
    "                question_id = question_ids_by_user[user_id][answered_height + i]\n",
    "                axes[0].annotate(question_id, (progress_df.index.get_loc(user_id), current_height + 0.5), \n",
    "                                 ha='center', va='center', fontsize=8, color=annotation_color, fontweight='bold')\n",
    "                current_height += 1\n",
    "\n",
    "        if context_unable_to_process_height > 0:\n",
    "            for i in range(context_unable_to_process_height):\n",
    "                question_id = question_ids_by_user[user_id][answered_height + error_height + i]\n",
    "                axes[0].annotate(question_id, (progress_df.index.get_loc(user_id), current_height + 0.5), \n",
    "                                 ha='center', va='center', fontsize=8, color=annotation_color, fontweight='bold')\n",
    "                current_height += 1\n",
    "\n",
    "        if llm_unable_to_process_height > 0:\n",
    "            for i in range(llm_unable_to_process_height):\n",
    "                question_id = question_ids_by_user[user_id][answered_height + error_height + context_unable_to_process_height + i]\n",
    "                axes[0].annotate(question_id, (progress_df.index.get_loc(user_id), current_height + 0.5), \n",
    "                                 ha='center', va='center', fontsize=8, color=annotation_color, fontweight='bold')\n",
    "                current_height += 1  # This line ensures proper spacing for yellow segments\n",
    "\n",
    "        if total_height > 0:\n",
    "            x = progress_df.index.get_loc(user_id)\n",
    "            time_text = f\"{user_time[user_id] / 60:.2f} min\"\n",
    "            axes[0].annotate(time_text, (x, total_height), ha='center', va='bottom', fontsize=10, color='black', fontweight='bold')\n",
    "\n",
    "    total_time_taken = (time.time() - start_time) / 60\n",
    "\n",
    "    total_errors = sum(user['error'] for user in user_progress.values())\n",
    "    total_successes = sum(user['answered'] for user in user_progress.values())\n",
    "    total_context_unable_to_process = sum(user['context_unable_to_process'] for user in user_progress.values())\n",
    "    total_llm_unable_to_process = sum(user['llm_unable_to_process'] for user in user_progress.values())\n",
    "    textstr = f'Total Errors: {total_errors}\\nTotal Successful Answers: {total_successes}\\nTotal Context Unable to Process: {total_context_unable_to_process}\\nTotal LLM Unable to Process: {total_llm_unable_to_process}\\nTotal Time Taken: {total_time_taken:.2f} min\\nNumber inside bar represent question ID.'\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "    axes[0].text(0.05, 0.95, textstr, transform=axes[0].transAxes, fontsize=8, verticalalignment='top', bbox=props)\n",
    "\n",
    "    axes[1].cla()\n",
    "    axes[1].plot(response_times, label='Response Time', color='orange')\n",
    "    axes[1].plot(avg_response_times, label='Average Response Time', linestyle='--', color='blue')\n",
    "    axes[1].set_xlabel('Request Number')\n",
    "    axes[1].set_ylabel('Response Time (s)')\n",
    "    axes[1].set_title('Response Times')\n",
    "    axes[1].legend()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "\n",
    "def analyze_results(user_count, question_count, total_successes, total_errors, total_unable_to_process, total_time_taken, avg_response_times, response_times, error_log):\n",
    "    bedrock_client = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\",\n",
    "        \n",
    "    )\n",
    "\n",
    "    llm = BedrockLLM(\n",
    "        model_id=model_id,\n",
    "        client=bedrock_client,\n",
    "        model_kwargs={\n",
    "            \"max_gen_len\": 1024,\n",
    "            \"temperature\": 0.3,  # Decreased temperature for more deterministic responses\n",
    "            \"top_p\": 0.9,  # Added top_p for more diverse responses\n",
    "        }\n",
    "    )\n",
    "\n",
    "    summary_template = \"\"\"\n",
    "    Volume Testing Results Summary:\n",
    "    - Number of users: {user_count}\n",
    "    - Number of questions: {question_count}\n",
    "    - Total Successful Answers: {total_successes}\n",
    "    - Total Errors: {total_errors}\n",
    "    - Total Unable to Process: {total_unable_to_process}\n",
    "    - Total Time Taken: {total_time_taken:.2f} minutes\n",
    "    - Average Response Time: {avg_response_time:.2f} seconds\n",
    "    - Maximum Response Time: {max_response_time:.2f} seconds\n",
    "    - Minimum Response Time: {min_response_time:.2f} seconds\n",
    "    - Standard Deviation of Response Times: {std_dev_response_time:.2f} seconds\n",
    "\n",
    "    Error Log:\n",
    "    {error_log}\n",
    "    \"\"\"\n",
    "\n",
    "    question_template = \"\"\"\n",
    "    We are testing an API. Please analyze the following volume test results and provide insights on what is good, what problems exist, and how to improve performance if there are any issues.\n",
    "\n",
    "    {results_summary}\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate standard deviation of response times\n",
    "    std_dev_response_time = statistics.stdev(response_times)\n",
    "\n",
    "    results_summary = summary_template.format(\n",
    "        user_count=user_count,\n",
    "        question_count=question_count,\n",
    "        total_successes=total_successes,\n",
    "        total_errors=total_errors,\n",
    "        total_unable_to_process=total_unable_to_process,\n",
    "        total_time_taken=total_time_taken,\n",
    "        avg_response_time=sum(avg_response_times) / len(avg_response_times),\n",
    "        max_response_time=max(response_times),\n",
    "        min_response_time=min(response_times),\n",
    "        std_dev_response_time=std_dev_response_time,\n",
    "        error_log=error_log\n",
    "    )\n",
    "\n",
    "    question = question_template.format(results_summary=results_summary)\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"{question}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        bedrock_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "        response = bedrock_chain({'question': question})\n",
    "        return response['text']\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during analysis: {str(e)}\")\n",
    "        return f\"An error occurred during analysis: {str(e)}\"\n",
    "\n",
    "async def main():\n",
    "    users_file_path = \"wex-users.xlsx\"\n",
    "    questions_file_path = \"wex-questions.xlsx\"\n",
    "\n",
    "    is_users_valid, user_count = validate_file(users_file_path)\n",
    "    is_questions_valid, question_count = validate_file(questions_file_path)\n",
    "\n",
    "    if not is_users_valid or not is_questions_valid:\n",
    "        print(\"One or both of the uploaded files are not valid Excel files.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    users = pd.read_excel(users_file_path)\n",
    "    user_ids = users[\"user-id\"].tolist()\n",
    "    client_ids = users[\"client-id\"].tolist()\n",
    "\n",
    "    user_tokens = {}\n",
    "\n",
    "    tasks = [retrieve_token(user_id, client_id, AUTH_CLIENT_SECRET, AUTH_URL) for user_id, client_id in zip(user_ids, client_ids)]\n",
    "    tokens = await asyncio.gather(*tasks)\n",
    "\n",
    "    for user_id, token in zip(user_ids, tokens):\n",
    "        if token and await validate_token(token):\n",
    "            user_tokens[user_id] = token\n",
    "        else:\n",
    "            print(f\"Error: Invalid Bearer token for user {user_id}.\")\n",
    "\n",
    "    if not user_tokens:\n",
    "        print(\"No valid tokens retrieved. Exiting...\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    error_log = []\n",
    "    response_times = []\n",
    "    avg_response_times = []\n",
    "    results = []\n",
    "\n",
    "    questions_df = pd.read_excel(questions_file_path)\n",
    "    questions = questions_df[\"Question\"].tolist()\n",
    "    question_ids = questions_df[\"question-id\"].tolist()\n",
    "\n",
    "    user_progress = {user_id: {\"answered\": 0, \"error\": 0, \"context_unable_to_process\": 0, \"llm_unable_to_process\": 0} for user_id in user_ids}\n",
    "    user_time = {user_id: 0 for user_id in user_ids}\n",
    "    lock = Lock()\n",
    "\n",
    "    fig, axes = initialize_plot()\n",
    "    display(fig)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    tasks = [\n",
    "        simulate_user_questions(user_id, questions, question_ids, user_progress, user_time, response_times, avg_response_times, error_log, lock, results, token, start_time, NUM_QUESTIONS_PER_USER, fig, axes)\n",
    "        for user_id, token in user_tokens.items()\n",
    "    ]\n",
    "    num_questions_answered_list = await asyncio.gather(*tasks)\n",
    "\n",
    "    total_questions_answered = sum(num_questions_answered_list)\n",
    "\n",
    "    total_time_taken = (time.time() - start_time) / 60\n",
    "\n",
    "    print(f\"Volume testing completed with {user_count} users and {total_questions_answered} questions.\")\n",
    "    if error_log:\n",
    "        print(\"Errors occurred during volume testing. Here are the details:\")\n",
    "        for error in error_log:\n",
    "            print(error)\n",
    "\n",
    "    try:\n",
    "        total_successes = sum(user['answered'] for user in user_progress.values())\n",
    "        total_errors = sum(user['error'] for user in user_progress.values())\n",
    "        total_unable_to_process = sum(user['context_unable_to_process'] for user in user_progress.values()) + sum(user['llm_unable_to_process'] for user in user_progress.values())\n",
    "        analysis = analyze_results(len(user_ids), total_questions_answered, total_successes, total_errors, total_unable_to_process, total_time_taken, avg_response_times, response_times, error_log)\n",
    "        print(\"\\nLLM Analysis and Recommendations:\\n\")\n",
    "        print(analysis)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during analysis: {str(e)}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=['user-id', 'question', 'answer', 'error', 'start_time', 'end_time', 'question_id'])\n",
    "    results_df['start_time'] = pd.to_datetime(results_df['start_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    results_df['end_time'] = pd.to_datetime(results_df['end_time']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    timestamp = datetime.now().strftime('%Y%m%d-%H%M')\n",
    "    results_df.to_excel(f'volume-test-run-{timestamp}.xlsx', index=False)\n",
    "    print(f\"Results saved to volume-test-run-{timestamp}.xlsx\")\n",
    "    \n",
    "    plt.close(fig)\n",
    "\n",
    "# Run the main function\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
