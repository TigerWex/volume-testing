{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import random\n",
    "import boto3\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import time\n",
    "import certifi\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from IPython.display import display, clear_output\n",
    "from threading import Lock\n",
    "\n",
    "# Set the AWS profile and certificate bundle\n",
    "os.environ[\"AWS_PROFILE\"] = \"default\"\n",
    "os.environ[\"REQUESTS_CA_BUNDLE\"] = certifi.where()\n",
    "\n",
    "# Set the model ID\n",
    "model_id = \"meta.llama3-8b-instruct-v1:0\"\n",
    "\n",
    "# Create a temporary directory for storing the uploaded files\n",
    "temp_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "def validate_file(file_path):\n",
    "    \"\"\"\n",
    "    Validates the uploaded file to ensure it can be read as an Excel file.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path: str, path to the file to be validated\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple (bool, int): True if the file is valid, and the number of rows in the file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        return True, df.shape[0]\n",
    "    except Exception as e:\n",
    "        return False, 0\n",
    "\n",
    "def initialize_bedrock_client():\n",
    "    \"\"\"\n",
    "    Initializes the Bedrock client.\n",
    "    \n",
    "    Returns:\n",
    "    - bedrock_client: boto3 client, initialized Bedrock client\n",
    "    \"\"\"\n",
    "    return boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "\n",
    "def chat(question):\n",
    "    \"\"\"\n",
    "    Sends a question to the Bedrock LLM and returns the response.\n",
    "    \n",
    "    Parameters:\n",
    "    - question: str, the question to be sent\n",
    "    \n",
    "    Returns:\n",
    "    - response: dict, the response from the LLM\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"{question}\"\n",
    "    )\n",
    "\n",
    "    bedrock_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    response = bedrock_chain({'question': question})\n",
    "    return response\n",
    "\n",
    "def simulate_user_questions(user_id, questions, user_progress, user_time, response_times, error_log, lock):\n",
    "    \"\"\"\n",
    "    Simulates a user asking questions and updates the progress.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_id: int, ID of the user\n",
    "    - questions: list, list of questions to ask\n",
    "    - user_progress: dict, dictionary to store user progress\n",
    "    - user_time: dict, dictionary to store time taken by the user\n",
    "    - response_times: list, list to store response times for each question\n",
    "    - error_log: list, list to store error messages\n",
    "    - lock: threading.Lock, lock to synchronize access to shared resources\n",
    "    \"\"\"\n",
    "    random.shuffle(questions)\n",
    "    for question in questions:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            response = chat(question)\n",
    "            answer = response[\"text\"]\n",
    "            end_time = time.time()\n",
    "            time_taken = end_time - start_time\n",
    "            user_time[user_id] += time_taken\n",
    "            response_times.append(time_taken)\n",
    "            user_progress[user_id][\"answered\"] += 1\n",
    "        except Exception as e:\n",
    "            error_info = f\"Error occurred for user {user_id} question: {question}. Error: {str(e)}\"\n",
    "            error_log.append(error_info)\n",
    "            user_progress[user_id][\"error\"] += 1\n",
    "\n",
    "        with lock:\n",
    "            update_plot(user_progress, user_time, response_times, error_log)\n",
    "\n",
    "def update_plot(user_progress, user_time, response_times, error_log):\n",
    "    \"\"\"\n",
    "    Updates the plot showing the progress of users and other metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - user_progress: dict, dictionary containing progress of users\n",
    "    - user_time: dict, dictionary containing time taken by users\n",
    "    - response_times: list, list of response times\n",
    "    - error_log: list, list of error messages\n",
    "    \"\"\"\n",
    "    progress_df = pd.DataFrame(user_progress).T\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(10, 20))\n",
    "\n",
    "    # Plot progress of answered questions for each user\n",
    "    progress_df.plot(kind='bar', stacked=True, color=['green', 'red'], ax=axes[0])\n",
    "    axes[0].set_xlabel('User ID')\n",
    "    axes[0].set_ylabel('Number of Questions')\n",
    "    axes[0].set_title('Progress of Answered Questions for Each User')\n",
    "    axes[0].yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    for p in axes[0].patches:\n",
    "        user_id = int(p.get_x() + p.get_width() / 2)\n",
    "        time_text = f\"time: {user_time[user_id] / 60:.2f} min\"\n",
    "        axes[0].annotate(time_text, (p.get_x() + p.get_width() / 2, p.get_height() + 0.5), \n",
    "                         ha='center', va='center', fontsize=10, color='black', fontweight='bold')\n",
    "\n",
    "    # Plot response times\n",
    "    axes[1].plot(response_times, label='Response Time')\n",
    "    axes[1].set_xlabel('Request Number')\n",
    "    axes[1].set_ylabel('Response Time (s)')\n",
    "    axes[1].set_title('Response Times')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Plot errors over time\n",
    "    error_times = list(range(len(error_log)))\n",
    "    axes[2].plot(error_times, [1] * len(error_log), 'r.', label='Error')\n",
    "    axes[2].set_xlabel('Request Number')\n",
    "    axes[2].set_ylabel('Errors')\n",
    "    axes[2].set_title('Errors Over Time')\n",
    "    axes[2].legend()\n",
    "\n",
    "    # Plot summary statistics\n",
    "    average_response_time = sum(response_times) / len(response_times) if response_times else 0\n",
    "    percentile_95_response_time = (pd.Series(response_times).quantile(0.95) if response_times else 0)\n",
    "    total_errors = len(error_log)\n",
    "    summary_data = {\n",
    "        'Metric': ['Average Response Time', '95th Percentile Response Time', 'Total Errors'],\n",
    "        'Value': [average_response_time, percentile_95_response_time, total_errors]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.plot(kind='bar', x='Metric', y='Value', ax=axes[3], legend=False, color='blue')\n",
    "    axes[3].set_title('Summary Statistics')\n",
    "    axes[3].set_ylabel('Value')\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(fig)\n",
    "    plt.close(fig)\n",
    "\n",
    "# Placeholder for user input file paths\n",
    "users_file_path = \"users.xlsx\"  # Replace with the actual path\n",
    "questions_file_path = \"questions.xlsx\"  # Replace with the actual path\n",
    "\n",
    "# Validate the files\n",
    "is_users_valid, user_count = validate_file(users_file_path)\n",
    "is_questions_valid, question_count = validate_file(questions_file_path)\n",
    "\n",
    "error_log = []\n",
    "response_times = []\n",
    "\n",
    "if is_users_valid and is_questions_valid:\n",
    "    users = pd.read_excel(users_file_path)\n",
    "    questions_df = pd.read_excel(questions_file_path)\n",
    "\n",
    "    if 'Question' not in questions_df.columns:\n",
    "        print(\"Error: The 'Question' column is not found in the questions file.\")\n",
    "        print(f\"Available columns are: {questions_df.columns.tolist()}\")\n",
    "    else:\n",
    "        questions = questions_df[\"Question\"].tolist()\n",
    "\n",
    "        user_progress = {user_id: {\"answered\": 0, \"error\": 0} for user_id in users.index}\n",
    "        user_time = {user_id: 0 for user_id in users.index}\n",
    "        lock = Lock()\n",
    "\n",
    "        bedrock_client = initialize_bedrock_client()\n",
    "\n",
    "        llm = BedrockLLM(\n",
    "            model_id=model_id,\n",
    "            client=bedrock_client,\n",
    "            model_kwargs={\"max_gen_len\": 512, \"temperature\": 0.5}\n",
    "        )\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=user_count) as executor:\n",
    "            futures = []\n",
    "            for user_id in users.index:\n",
    "                futures.append(executor.submit(simulate_user_questions, user_id, questions, user_progress, user_time, response_times, error_log, lock))\n",
    "                time.sleep(5)\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                future.result()\n",
    "\n",
    "        print(f\"Volume testing completed with {user_count} users and {question_count} questions.\")\n",
    "        if error_log:\n",
    "            print(\"Errors occurred during volume testing. Here are the details:\")\n",
    "            for error in error_log:\n",
    "                print(error)\n",
    "else:\n",
    "    print(\"One or both of the uploaded files are not valid Excel files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
